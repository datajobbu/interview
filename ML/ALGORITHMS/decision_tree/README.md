## Decision Tree 란?

한번에 하나씩의 설명변수를 사용하여 예측 가능한 규칙들의 집합을 생성하는 알고리즘입니다.

**장점:**

- 특성의 스케일을 맞추거나 평균을 원점에 맞추는 작업이 필요하지 않습니다.
- 시간 복잡도가 $O(log_2(m))$으로 데이터가 커도 빠른 연산이 가능합니다.
- 이해하고 해석하기 쉽습니다.
- 사용하기 쉽습니다.
- 여러 용도(Regressor, Classifier)로 사용할 수 있습니다.
- 성능이 뛰어납니다.

**단점:**

- 계단 모양의 결정 경계를 만들어(모든 분할은 축에 수직) 훈련 셋의 회전에 민감합니다. → PCA 통해 해결 가능
- 훈련 데이터에 있는 작은 변화에도 매우 민감합니다.

---

## Criterion

### Classifier

Classifier의 기본 criterion으로는 지니 불순도가 사용되지만 "entropy"로 지정하여 엔트로피 불순도를 사용할 수 있습니다. 

**GINI Impurity**

한 노드의 모든 샘플이 같은 클래스에 속해 있다면 이 노드를 순수($gini=0$)하다고 합니다. $i$번째 노드의 gini 점수 $G_i$ 를 계산하는 식은 아래와 같습니다.

$$G_i = 1-\sum_{k=1}^{n}{P_{i,k}}^2$$

- 이때 $P_{i,k}$ 는 $i$번째 노드에 있는 훈련 샘플 중 클래스 $k$에 속한 샘플의 비율입니다.

**Entropy**

분자의 무질서함을 측정하는 것으로 원래 열역학의 개념입니다. 분자가 안정되고 질서 정연하면 엔트로피가 0에 가깝습니다(정보학적 입장에선 얻을 정보량이 없다). 메시지의 평균 정보 양을 측정하는 섀년의 정보 이론도 엔트로피에서 비롯되었습니다. 여기서는 모든 메시지가 동일할 때 엔트로피가 0이 됩니다. 머신러닝에서는 불순도의 측정 방법으로 자주 사용됩니다. 어떤 세트가 한 클래스의 샘플만 담고 있다면 엔트로피가 0입니다. $i$번째 노드의 엔트로피 계산 식은 아래와 같습니다.

$$H_i = - \sum_{k=1}^{n}P_{i,k}log_2(P_{i,k})$$

- 이때, $P_{i,k}$는 0이 아닌 값입니다.

그렇다면 지니와 엔트로피 중 어떤 것을 사용해야 할까요? 실제로는 큰 차이가 없습니다. 둘다 비슷한 트리를 만들어냅니다. 지니 불순도가 조금 더 계산이 빠르기 때문에 기본값으로 좋습니다. 그러나 다른 트리가 만들어지는 경우 지니 불순도가 가장 빈도 높은 클래스를 한쪽 가지로 고립시키는 경향이 있는 반면 엔트로피는 조금 더 균형 잡힌 트리를 만듭니다. [[참고]](https://goo.gl/YbjeNx)

### Regressor

Classifier와 거의 유사하지만 주요한 차이는 각 노드에서 클래스를 예측하는 대신 어떤 값을 예측한다는 점입니다. 각 영역의 예측값은 항상 그 영역에 있는 타깃값의 평균이 됩니다. 알고리즘은 예측값과 가능한 한 많은 샘플이 가까이 있도록 영역을 분할합니다.

**MSE**

$$MSE_{node} = \sum_{i \in node}(\hat y_{node} - y^{(i)})^2$$

$$\hat y_{node} = \frac{1}{m_{node}}\sum_{i \in node} y^{(i)}$$

`CART` 알고리즘은 Training Set을 불순도를 최소화하는 방향으로 분할하는 대신 평균제곱오차(MSE)를 최소화하도록 분할합니다.

---

## 클래스 확률 추정

Decision Tree는 각 클래스에 속할 확률을 구할 수 있습니다. 코드 상에서 `predict_proba()` 를 이용하면 각 클래스에 속할 확률을 구해주고, `predict()` 를 이용하면, 확률 중 가장 큰 확률을 갖는 클래스의 index를 반환합니다.

---

## Training Algorithms

### CART(Classification And Regression Tree)

사이킷런은 Decision Tree를 훈련시키기 위해 `CART` 알고리즘을 사용합니다. `CART` 는 기본적으로 `탐욕적(Greedy)` 알고리즘 입니다. 맨 위 루트 노드에서 최적의 분할을 찾으며 이어지는 각 단계에서 이 과정을 반복합니다. 현재 단계의 분할이 몇 단계를 거쳐 가장 낮은 불순도로 이어질 수 있을지 없을지는 고려하지 않습니다. 탐욕적 알고리즘은 종종 납득할 만한 훌륭한 솔루션을 만들어내지만, 최적의 솔루션을 보장하지는 않습니다. 불행하게도 최적의 트리를 찾는 것은 `NP-Complete` 문제로 알려져 있습니다. 이 문제는 $O(log_2(m))$ 시간이 필요하고 매우 작은 Training Set에도 적용하기 어렵습니다. 그러므로 '납득할 만한 좋은 솔루션'으로만 만족해야 합니다.

`CART` 알고리즘은 먼저 Training Set을 하나의 특성 $k$의 임계값 $t_k$를 사용해 두 개의 Subset으로 나눕니다. 그렇다면 어떻게 $k$와 $t_k$를 고를까요? (크기에 따라 가중치가 적용된) 가장 순수한 Subset으로 나눌 수 있는 $(k, t_k)$ 짝을 찾습니다. 이 알고리즘이 최소화해야하는 비용 함수는 아래와 같습니다.

**Classifier 비용 함수**

$$J(k,t_k) = \frac{m_{left}}{m}G_{left} + \frac{m_{right}}{m}G_{right} $$

**Regressor 비용 함수**

$$J(k,t_k) = \frac{m_{left}}{m}MSE_{left} + \frac{m_{right}}{m}MSE_{right}$$

- 이때 $G$는 subset의 불순도, $m$은 subset의 샘플 수입니다.

`CART` 알고리즘이 Training Set을 성공적으로 둘로 나누었다면, 같은 방식으로 subset을 또 나누고 그다음엔 subset의 subset을 나누고 이런 식으로 계속 반복하게 됩니다. 이 과정은 최대 깊이(`max_depth`

)가 되면 중지하거나 불순도를 줄이는 분할을 찾을 수 없을 때 멈추게 됩니다. 

이진 트리만 만드는 알고리즘

### ID3

둘 이상의 자식 노드를 가진 결정 트리 만들 수 있음

---

## 계산 복잡도

예측을 하려면 Decision Tree를 루트 노드에서부터 리프 노드까지 탐색해야 합니다. 일반적으로 Decision Tree는 거의 균형을 이루고 있으므로 Decision Tree를 탐색하기 위해서는 $O(log_2(m))$개의 노드를 거쳐야 합니다**. 각 노드는 하나의 특성값만 확인하기 때문에 예측에 필요한 전체 복잡도는 특성 수와 무관하게 $O(log_2(m))$입니다. 그래서 큰 Training Set을 다룰 때에도 예측 속도가 매우 빠릅니다. 

훈련 알고리즘은 각 노드에서 모든 훈련 샘플의 모든(또는 `max_features` 가 지정되었다면 그보다는 적은) 특성을 비교합니다. 각 노드에서 모든 샘플의 모든 특성을 비교하면 훈련 복잡도는 $O(n \times mlog_2(m))$이 됩니다. Training Set이 (수천 개 이하의 샘플 정도로) 작을 경우 사이킷런은 (`presort=True` 로 지정하면) 미리 데이터를 정렬하여 훈련 속도를 높일 수 있습니다***. 하지만 Training Set이 클 경우에는 속도가 많이 느려집니다.

** 균형 이진 트리에서 깊이 d에서의 리프 노드의 개수는 2^d 이다. 리프 노드가 훈련 데이터 수(m)만큼 있다면 이 트리의 깊이는 log2(m)이 된다.

*** 결정 트리에서 사용하는 퀵 정렬이 $O(m log(m))$의 복잡도를 가지고 특성별로 정렬을 수행해야 하므로 전체 복잡도는 $O(n \times m log(m))$이 됩니다.

---

## 규제 매개변수

결정 트리는 훈련 데이터에 대한 제약 사항이 거의 없습니다. 제한을 두지 않으면 트리가 훈련 데이터에 아주 가깝게 맞추려고 해서 대부분 Overfitting되기 쉽습니다. Decision Tree는 모델 파라미터가 전혀 없는 것이 아니라(보통 많습니다) 훈련되기 전에 파라미터 수가 결정되지 않기 때문에 이런 모델을 'Nonparametric Model'이라고 부르곤 합니다. 그래서 모델 구조가 데이터에 맞춰져서 고정되지 않고 자유롭습니다. 반대로 선형 모델 같은 'Parametric Model'은 미리 정의된 모델 파라미터 수를 가지므로 자유도가 제한되고 Overfitting될 위험이 줄어듭니다(하지만 Underfitting될 위험은 커집니다).

훈련 데이터에 대한 과대적합을 피하기 위해 학습할 때 결정 트리의 자유도를 제한할 필요가 있습니다. 이미 알고 있듯이 이를 규제하고 합니다. 규제 매개변수는 사용하는 알고리즘에 따라 다르지만, 보통 적어도 결정 트리의 최대 깊이는 제어할 수 있습니다. 사이킷런에서는 `max_depth` 매개변수로 이를 조절합니다(`default=None`).

이외에도 DecisionTreeClassifier에 있는 규제 매개변수는 아래와 같습니다.

- `min_samples_split` - 분할되기 위해 노드가 가져야 하는 최소 샘플 수
- `min_samples_leaf` - 리프 노드가 가지고 있어야 할 최소 샘플 수
- `min_weight_fraction_leaf` - 가중치가 부여된 전체 샘플 수에서의 비율
- `max_leaf_nodes` - 리프 노드의 최대 수
- `max_features` - 각 노드에서 분할에 사용할 특성의 최대 수

** 제한 없이 Decision Tree를 학습시키고 불필요한 노드를 가지치기(Pruning)하는 알고리즘도 있습니다(Post-Pruning - 사이킷런은 트리 생성을 미리 제한하는 Pre-Pruning만 지원합니다). 순도를 높이는 것이 통계적으로 큰 효과가 없다면 리프 노드 바로 위의 노드는 불필요할 수 있습니다. 대표적으로 $x^2$ 검정(Chi-squared Test) 같은 통계적 검정을 사용하여 우연히 향상된 것인지 추정합니다(귀무가설이라 부릅니다). 이 확률을 p-value라 부르며 어떤 임곗값(하이퍼파라미터로 조정되지만 통상적으로 5%)보다 높으면 그 노드는 불필요한 것으로 간주되고 그 자식 노드는 삭제됩니다. 가지치기는 불필요한 노드가 모두 없어질 때까지 계속됩니다.

---

## 연습문제

1. **백만 개의 샘플을 가진 Training Set에서 (규제 없이) 훈련시킨 Decision Tree의 깊이는 대략 얼마일까요?**

    → 이진 결정 트리를 제한을 두지 않고 훈련시키면 훈련 샘플마다 하나의 리프 노드가 되므로 어느 정도 균형이 잡힌 트리가 됩니다. 이 경우 이진 트리의 깊이는 $log_2(m)$을 반올림한 것과 같습니다. 따라서 훈련 세트에 백만 개 샘플이 있다면 결정 트리의 깊이는 $log_2(10^6) \approx 20$이 될 것입니다. 이는 이상적인 값이고 대체적으로 더 크게 나옵니다.

2. **한 노드의 지니 불순도가 보통 그 부모 노드보다 작을까요, 아니면 클까요? 일반적으로 작거나 클까요, 아니면 항상 작거나 클까요?**

    → 한 노드의 지니 불순도는 일반적으로 부모의 불순도보다 낮습니다. 이는 자식의 지니 불순도의 가중치 합이 최소화되는 방향으로 각 노드를 분할하는 `CART` 알고리즘의 비용 함수 때문입니다. 그러나 다른 자식 노드의 지니 불순도 감소량이 어떤 노드의 불순도 증가량보다 큰 경우라면 부모의 불순도가 큰 노드가 생길 수 있습니다.

3. **Decision Tree가 Training Set에 Underfitting 되었다면 입력 특성의 스케일을 조정하는 것이 좋을까요?**

    → 결정 트리는 훈련 데이터의 스케일이나 원점에 맞추어져 있는지 상관하지 않습니다. 따라서 입력 특성의 스케일을 조정하는 것은 큰 의미가 없습니다.

4. **백만 개의 샘플을 가진 Training Set에 Decision Tree를 훈련시키는 데 한 시간이 걸렸다면, 천만 개의 샘플을 가진 Training Set에 결정 트리를 훈련시키는 데는 대략 얼마나 걸릴까요?**

    → 결정 트리 훈련의 계산 복잡도는 $O(n \times m log(m))$입니다. 그러므로 훈련 세트의 크기에 10을 곱하면 훈련 시간은 $10 \times log(10m) / log(m)$배 늘어납니다. 만약 $m=10^6$이면 $K \approx 11.7$이므로 훈련에 대략 11.7시간이 걸릴 것으로 예상할 수 있습니다.

5. **십만 개의 샘플을 가진 훈련 세트가 있다면 `presort=True` 로 지정하는 것이 훈련 속도를 높일까요?**

    → 수천 개 미만일 경우에는 훈련 속도를 높일 수 있지만, 100,000개의 경우에는 속도가 매우 느려질 것입니다.

---

## 참고자료

BASE = 핸즈온 머신러닝[2판]

[의사결정나무(Decision Tree)](https://ratsgo.github.io/machine%20learning/2017/03/26/tree/)