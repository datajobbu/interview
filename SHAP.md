# SHAP(SHapley Additive exPlanations)

Lundburg와 Lee가 제안한 SHAP은 각 예측치를 설명할 수 있는 방법이다.

로이드 섀플리가 만든 이론 위에 피처 간 독립성을 근거로 덧셈이 가능하게 활용도를 넓힌 논문이다.



## 1. Shapley Value

섀플리 값은 전체 성과를 창출하는 데 각 참여자가 얼마나 공헌했는지를 수치로 표현한 값이다.

>  각 사람(변수)의 기여도 = 그 사람의 기여도를 제외했을 때 전체 성과의 변화 정도

섀플리 값은 음수일 수 있으며, 이 경우 특정 피처가 예측에 부정적인 영향을 미치고 있다고 해석할 수 있다.



## 2. SHAP

SHAP는 Shapley value의 계산 방법을 기반으로 하여 데이터 셋의 전체적인 영역을 해석할 수 있는 많은 방법을 가지고 있다.

SHAP는 모델의 출력을 각 피처의 기여도로 분해한다.

F모델은 Featrue Importance의 사용을 지양해야 한다.

하지만, SHAP은 피처 간 의존성까지 고려해서 모델 영향력을 계산한다.

SHAP가 계산한 모든 피쳐의 영향력 합은 1(100%)이다.

SHAP은 계산 시간이 오래 걸린다 - 피처의 결측을 시뮬레이션해야 하기 떄문이다.

Feature Importance와 부분 의존성 플롯의 경우 모델의 관점에 대한 설명이지만, SHAP은 데이터 하나에 대한 설명을 구하는 것이다.



* Kernel SHAP

* Tree SHAP

  

## 3. Feature Importance와 ABS(SHAP) 비교

얼핏보면 feature importance와 SHAP는 비슷해 보인다.

하지만 실행 결과는 꽤 다르다.

<br/>

Featrue Importance는 Permutation 기법을 사용해서 피처가 모델에 미치는 영향을 측정한다.

<br/>

단점:

- Feature Importance는 피처 간 의존성을 간과하여 피처 간에 상관관계가 존재하는 경우 결과가 왜곡될 수 있다.

- 음의 영향력은 계산하지 않는다.(즉, 어떤 피처가 결과에 부정적인 영향을 미쳐도 피처 중요도에는 반영되지 않는다.) 
  따라서 피처 중요도 기법은 **실제 영향력보다 특정 피처의 가치가 높게 책정**될 수 있다.

이러한 단점에도 불구하고 피처 중요도는 측정하기 쉽고 알고리즘이 단순해서 모델의 간편한 이해를 위해 사용된다.

<br/>

반면 섀플리 값은 피처들이 서로 영향을 미칠 가능성을 고려한다.

<br/>

단점:

* SHAP는 기본 계산량이 많아서 측정 속도가 느리다.(데이터 수의 제곱에 비례해 계산량이 증가한다.)
  느리다고 기본 계산량을 줄이면 오차 분산이 커져 신뢰도가 떨어진다.
* SHAP는 기존의 데이터를 확고하게 믿고 해석하기 때문에 아웃라이어 데이터에 취약하다.

장점:

- SHAP는 피처 간의 의존성을 고려하고 음의 영향력을 계산할 수 있다.
  따라서 섀플리 값은 피처 중요도가 보장하지 못하는 넓은 범위의 함정을 피할 수 있다.

<br/>

항상은 아니겠지만, 대체적으로 featrue importance 보다는 SHAP를 더 신뢰할만 하다.

